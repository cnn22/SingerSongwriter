{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnn22/SingerSongwriter/blob/main/NMT_Attention_Marathi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFjQytjDYrE"
      },
      "source": [
        "#### Dataset => http://www.manythings.org/anki/ (mar-eng.zip)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Stuff"
      ],
      "metadata": {
        "id": "FJ4ntpvUGV1w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KiCIi68kDYrI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import string\n",
        "\n",
        "# import plotly\n",
        "# import plotly.plotly as py\n",
        "# from plotly.offline import init_notebook_mode, iplot\n",
        "# plotly.offline.init_notebook_mode(connected=True)\n",
        "# import plotly.graph_objs as go"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess\n",
        "Removing digits, punctuation, and other non-related items from the sentences"
      ],
      "metadata": {
        "id": "6LNpTf-LGX4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mEflZmwCDYrL"
      },
      "outputs": [],
      "source": [
        "file_path = './spa.txt' # please set the path according to your system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "3rmcjLIkDYrM",
        "outputId": "58fae5a3-718f-4c54-a905-6be6f2d62618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.\\tVe.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986655 (cueyayotl)',\n",
              " 'Go.\\tVete.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986656 (cueyayotl)']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "lines = open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
        "lines[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZBk1eGRaDYrM",
        "outputId": "4bf11c73-e2ad-4f2b-9386-32e64878dd65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "139013"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bzGA9juzDYrN"
      },
      "outputs": [],
      "source": [
        "exclude = set(string.punctuation) # Set of all special characters to remove from lines\n",
        "remove_digits = str.maketrans('', '', string.digits) # Set of all digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OdEV_shRDYrN"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sent):\n",
        "    '''Function to preprocess English sentence'''\n",
        "    sent = sent.lower() # lower casing\n",
        "    sent = re.sub(\"'\", '', sent) # remove the quotation marks if any\n",
        "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
        "    sent = sent.translate(remove_digits) # remove the digits\n",
        "    sent = sent.strip()\n",
        "    sent = re.sub(\" +\", \" \", sent) # remove extra spaces\n",
        "    sent = '<start> ' + sent + ' <end>' # add <start> and <end> tokens\n",
        "    return sent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_sentence(\"fasadf```adf12()312341;23fafasdf`1231231\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0uOeQTgdFa33",
        "outputId": "8b9d2253-3e0e-438f-99e0-67e847fa47bd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> fasadfadffafasdf <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wB4fLomtDYrP",
        "outputId": "5c540a6e-7261-4fd6-f1ef-8f1abb88cdee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<start> hes no saint <end>', '<start> él no es un santo <end>'],\n",
              " ['<start> hes not home <end>', '<start> no está en casa <end>'],\n",
              " ['<start> hes not sick <end>', '<start> no está enfermo <end>'],\n",
              " ['<start> hes studying <end>', '<start> él está estudiando <end>'],\n",
              " ['<start> hes your son <end>', '<start> él es tu hijo <end>'],\n",
              " ['<start> heat the milk <end>', '<start> calienta la leche <end>'],\n",
              " ['<start> help yourself <end>', '<start> sírvase usted mismo <end>'],\n",
              " ['<start> help yourself <end>', '<start> sírvete tú mismo <end>'],\n",
              " ['<start> help yourself <end>', '<start> servíos vosotros mismos <end>'],\n",
              " ['<start> help yourself <end>', '<start> sírvanse ustedes mismos <end>']]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Generate pairs of cleaned English and Marathi sentences\n",
        "sent_pairs = []\n",
        "for line in lines:\n",
        "    sent_pair = []\n",
        "    eng, spa, trash = line.split('\\t')\n",
        "    eng = preprocess_sentence(eng)\n",
        "    sent_pair.append(eng)\n",
        "    spa = preprocess_sentence(spa)\n",
        "    sent_pair.append(spa)\n",
        "    sent_pairs.append(sent_pair)\n",
        "sent_pairs[5000:5010]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating dataset"
      ],
      "metadata": {
        "id": "xR8iXxzwGi5G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4X9IMnD5DYrQ"
      },
      "outputs": [],
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "\n",
        "        self.create_index()\n",
        "\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1\n",
        "\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_myNLk4KDYrR"
      },
      "outputs": [],
      "source": [
        "def max_length(tensors):\n",
        "    return max(len(t) for t in tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SuZ5CEYiDYrR"
      },
      "outputs": [],
      "source": [
        "def load_dataset(pairs, num_examples):\n",
        "    # pairs => already created cleaned input, output pairs\n",
        "\n",
        "    # index language using the class defined above: Example word to index = \"day\": 5, index to word = 5:\"day\"    \n",
        "    inp_lang = LanguageIndex(en for en, sp in pairs) #English\n",
        "    targ_lang = LanguageIndex(sp for en, sp in pairs) #Spanish \n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    # English sentences\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
        "    print(\"Input Tensors\")\n",
        "    print(input_tensor[0:2])\n",
        "    # Spanish sentences\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
        "    print(\"Targets Tensors\")\n",
        "    print(target_tensor[0:2])\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    print(\"Input Tensor Max Length\")\n",
        "    print(max_length_inp)\n",
        "\n",
        "    print(\"Target Tensor Max Length\")\n",
        "    print(max_length_tar)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    # Making sure all of the items on the list are the same size as the max len\n",
        "    # For example:  [1,2,3] becomes [1,2,3,0,0,0] if the max is 6\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    print(\"Input Tensors after Tensorflow Pad Sequence\")\n",
        "    print(input_tensor[0:2])\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    print(\"Targets Tensors after Tensorflow Pad Sequence\")\n",
        "    print(target_tensor[0:2])\n",
        "\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Qzdcd27RDYrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd7a342-bb57-425b-9fc6-22c7842bc448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensors\n",
            "[[2, 5454, 1], [2, 5454, 1]]\n",
            "Targets Tensors\n",
            "[[2, 26684, 1], [2, 26915, 1]]\n",
            "Input Tensor Max Length\n",
            "72\n",
            "Target Tensor Max Length\n",
            "70\n",
            "Input Tensors after Tensorflow Pad Sequence\n",
            "[[   2 5454    1    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   2 5454    1    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "Targets Tensors after Tensorflow Pad Sequence\n",
            "[[    2 26684     1     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0]\n",
            " [    2 26915     1     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "kCX5hmFoDYrS",
        "outputId": "d9bfbb96-d851-4a5e-ca2b-5bfd821eb7ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(125111, 125111, 13902, 13902)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state = 101)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "8vL8Ty5mDYrT"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train) #traning dataset\n",
        "BATCH_SIZE = 64 #how many of the BUFFER_SIZE it will take per batch\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE #how many batch it will take: size of training divided by the batch size double // means no decimal\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx) #size of english vocab\n",
        "vocab_tar_size = len(targ_lang.word2idx) #size of spanish vocab\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It's learning time"
      ],
      "metadata": {
        "id": "hKKUcOiTM1Zs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "JeuQuaa5DYrT"
      },
      "outputs": [],
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "    if tf.test.is_gpu_available():\n",
        "        return tf.keras.layers.CuDNNGRU(units, \n",
        "                                        return_sequences=True, \n",
        "                                        return_state=True, \n",
        "                                        recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "        return tf.keras.layers.GRU(units, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True, \n",
        "                                   recurrent_activation='sigmoid', \n",
        "                                   recurrent_initializer='glorot_uniform')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pctNO-trDYrU"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz #how many items per batch\n",
        "        self.enc_units = enc_units #ENCODER UNITS not incremental units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #Embedding layer based on embedding_dim\n",
        "        self.gru = gru(self.enc_units) #Creating GRU for the Encoder Unit\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x) #run the embedding layer on x\n",
        "        output, state = self.gru(x, initial_state = hidden) #run gru on x        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units)) #init the starting hidden state mostly zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePByGlsxDYrU"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz #how many items per batch\n",
        "        self.dec_units = dec_units #DECODER UNITS not decremental units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #Embedding layer based on embedding_dim\n",
        "        self.gru = gru(self.dec_units) #Creating GRU for the Encoder Unit\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size) #Creating a Full Connected layer that does linear and non-linear trasnformation\n",
        "        #https://towardsdatascience.com/convolutional-layers-vs-fully-connected-layers-364f05ab460b#:~:text=Fully%20Connected%20Layers%20(FC%20Layers,vector%20through%20a%20weights%20matrix\n",
        "        \n",
        "        # used for attention: Bahdanau's Attention Linear Algebra\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units) \n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1) #v is weight vector \n",
        "        \n",
        "    #https://blog.floydhub.com/attention-mechanism/\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiDpVmPSDYrV"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX3IWeIIDYrV"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = 1 - np.equal(real, 0)\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23n_jy89DYrW"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQjNX-EtDYrW",
        "outputId": "8b41f5a8-caad-42ce-dcc4-1f9b6fddd7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.5206\n",
            "Epoch 1 Batch 100 Loss 1.0305\n",
            "Epoch 1 Batch 200 Loss 0.7886\n",
            "Epoch 1 Batch 300 Loss 0.7676\n",
            "Epoch 1 Batch 400 Loss 0.7481\n",
            "Epoch 1 Loss 0.8671\n",
            "Time taken for 1 epoch 352.7763113975525 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.7193\n",
            "Epoch 2 Batch 100 Loss 0.6355\n",
            "Epoch 2 Batch 200 Loss 0.6897\n",
            "Epoch 2 Batch 300 Loss 0.5928\n",
            "Epoch 2 Batch 400 Loss 0.5776\n",
            "Epoch 2 Loss 0.6535\n",
            "Time taken for 1 epoch 351.2787010669708 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.5011\n",
            "Epoch 3 Batch 100 Loss 0.4631\n",
            "Epoch 3 Batch 200 Loss 0.5464\n",
            "Epoch 3 Batch 300 Loss 0.5196\n",
            "Epoch 3 Batch 400 Loss 0.4962\n",
            "Epoch 3 Loss 0.4708\n",
            "Time taken for 1 epoch 353.4211723804474 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3234\n",
            "Epoch 4 Batch 100 Loss 0.3257\n",
            "Epoch 4 Batch 200 Loss 0.3345\n",
            "Epoch 4 Batch 300 Loss 0.3141\n",
            "Epoch 4 Batch 400 Loss 0.3244\n",
            "Epoch 4 Loss 0.3200\n",
            "Time taken for 1 epoch 353.1946771144867 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2030\n",
            "Epoch 5 Batch 100 Loss 0.2257\n",
            "Epoch 5 Batch 200 Loss 0.1945\n",
            "Epoch 5 Batch 300 Loss 0.2250\n",
            "Epoch 5 Batch 400 Loss 0.2372\n",
            "Epoch 5 Loss 0.2143\n",
            "Time taken for 1 epoch 354.23998522758484 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1239\n",
            "Epoch 6 Batch 100 Loss 0.1192\n",
            "Epoch 6 Batch 200 Loss 0.1396\n",
            "Epoch 6 Batch 300 Loss 0.1327\n",
            "Epoch 6 Batch 400 Loss 0.1459\n",
            "Epoch 6 Loss 0.1476\n",
            "Time taken for 1 epoch 354.23475980758667 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1021\n",
            "Epoch 7 Batch 100 Loss 0.0832\n",
            "Epoch 7 Batch 200 Loss 0.0870\n",
            "Epoch 7 Batch 300 Loss 0.1290\n",
            "Epoch 7 Batch 400 Loss 0.1195\n",
            "Epoch 7 Loss 0.1083\n",
            "Time taken for 1 epoch 355.2288661003113 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0767\n",
            "Epoch 8 Batch 100 Loss 0.0926\n",
            "Epoch 8 Batch 200 Loss 0.0877\n",
            "Epoch 8 Batch 300 Loss 0.1159\n",
            "Epoch 8 Batch 400 Loss 0.0882\n",
            "Epoch 8 Loss 0.0862\n",
            "Time taken for 1 epoch 352.97160029411316 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0666\n",
            "Epoch 9 Batch 100 Loss 0.0602\n",
            "Epoch 9 Batch 200 Loss 0.0587\n",
            "Epoch 9 Batch 300 Loss 0.0814\n",
            "Epoch 9 Batch 400 Loss 0.0761\n",
            "Epoch 9 Loss 0.0727\n",
            "Time taken for 1 epoch 353.38186383247375 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0606\n",
            "Epoch 10 Batch 100 Loss 0.0516\n",
            "Epoch 10 Batch 200 Loss 0.0664\n",
            "Epoch 10 Batch 300 Loss 0.0800\n",
            "Epoch 10 Batch 400 Loss 0.0683\n",
            "Epoch 10 Loss 0.0642\n",
            "Time taken for 1 epoch 353.1245322227478 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every epoch\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izTxnvtaDYrW",
        "outputId": "293f2306-8f60-4f2f-f936-d12e882e6e96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fe3b1759be0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO41jLhdDYrX"
      },
      "source": [
        "#### Inference Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1-CjzodDYrX"
      },
      "outputs": [],
      "source": [
        "def evaluate(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = ''\n",
        "    for i in inputs[0]:\n",
        "        if i == 0:\n",
        "            break\n",
        "        sentence = sentence + inp_lang.idx2word[i] + ' '\n",
        "    sentence = sentence[:-1]\n",
        "    \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwS4et-NDYrY"
      },
      "source": [
        "#### Function to predict (translate) a randomly selected test point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sox5Zuu-DYrY"
      },
      "outputs": [],
      "source": [
        "def predict_random_val_sentence():\n",
        "    actual_sent = ''\n",
        "    k = np.random.randint(len(input_tensor_val))\n",
        "    random_input = input_tensor_val[k]\n",
        "    random_output = target_tensor_val[k]\n",
        "    random_input = np.expand_dims(random_input,0)\n",
        "    result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "    print('Input: {}'.format(sentence[8:-6]))\n",
        "    print('Predicted translation: {}'.format(result[:-6]))\n",
        "    for i in random_output:\n",
        "        if i == 0:\n",
        "            break\n",
        "        actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
        "    actual_sent = actual_sent[8:-7]\n",
        "    print('Actual translation: {}'.format(actual_sent))\n",
        "    attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n",
        "    sentence, result = sentence.split(' '), result.split(' ')\n",
        "    sentence = sentence[1:-1]\n",
        "    result = result[:-2]\n",
        "    \n",
        "    # use plotly to generate the heat map\n",
        "    trace = go.Heatmap(z = attention_plot, x = sentence, y = result, colorscale='Reds')\n",
        "    data=[trace]\n",
        "    iplot(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NaYRON-DYrY"
      },
      "source": [
        "#### Call the function to visualize outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hizcY3fcDYrZ"
      },
      "outputs": [],
      "source": [
        "predict_random_val_sentence()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQsQ0bb_DYrZ"
      },
      "outputs": [],
      "source": [
        "predict_random_val_sentence()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZQg4rsnDYrZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO Section\n",
        "\n",
        "## TODO 08/05 \n",
        "* Reverse engineer this dude's work and understand how to calculate the scores (s1,s2, etc.) thats used to calculate the attention weights\n",
        "\n",
        "## TODO 09/23\n",
        "* On the 16th we walked through the dude's code up until the decoder layer where we learn that he used Bahdanau's paper as reference. We also learn about Full Connected Layer where it does a linear and non-linear activation on the input. \n",
        "* Learn Bahdanau Attention Algorithm more \n",
        "* Go through the rest of the dude's code \n",
        "\n",
        "* https://blog.floydhub.com/attention-mechanism/#bahdanau-att-step4\n",
        "* "
      ],
      "metadata": {
        "id": "JH80X_XmDbui"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqA9yLk9Du8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FJ4ntpvUGV1w"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}