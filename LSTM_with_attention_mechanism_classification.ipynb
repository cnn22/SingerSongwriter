{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnn22/SingerSongwriter/blob/main/LSTM_with_attention_mechanism_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "09/23\n",
        "\n",
        "*   Continue reading about how we can apply attention mechanism to our original LSTM model\n",
        "*   https://www.sentic.net/attention-based-bidirectional-cnn-rnn-for-sentiment-analysis.pdf \n",
        "* https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050922X00021/1-s2.0-S187705092200093X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFgaCXVzLWVhc3QtMSJIMEYCIQC1QZv6mipsjKjexv62MLLSFL6z7L6ZvUSsKjzWN1f3uAIhAIetz6FX3Et82RFNcHb6bJcqKnKfUSGJ6JCVVbGsfCK6KswECCEQBRoMMDU5MDAzNTQ2ODY1IgzqRFC2%2BwlaCJ6JNO0qqQSCLtdnIFfSCvuh81A%2FvAdHesX9M0l3R%2FQGsvZscw42Ffk7xr2S5ZprtRJKq%2BYWwKzZURHTE9hk9BZDIQ7A51jtleYLINTCHEKxG8Q60HY762aDl7l7JR%2Fmc5WCH40pDrsq0nfYS0NVCxLDzIy%2FjHo38UISDVeTo7G5GntET2PVWVbg3UpM73w8iCEzTnqsTiiQdyvIBqNPIyRLOy%2BENYbS4ICf0XxdwaucuS0FKZ1QsNXACHcVGIBj%2BUyOSSBZH4YaCYIKUYa0zP0RysYnff4I0wBzuwvsA2P%2FFbk59WprxM3vlZMbCnzX08IDZMbe9IghFhOfZ52Rve9nLoI%2ByV6k9ZbIqK6yoWl7aqY%2FKplFwhRolN12aFap1OsnYK8V7%2F%2BTJtYGPTZpgNgA68Ah3S0qSdCRTX4gzOvxbJ6pn%2B7PoH1BXT6i7IlxlcwBXGc1mgbYDfIh7tbOvZQFuDUUuLm2Vm2bKxcmIBE8lZM4rBF48CaBYuAza1JXJ0PafNbzM20s9BESnZeettmSNOSMvLNUBKtDHJDi5voT3qSaBAlE7R6Az9fRU2WwWeZj6pTO2TTD89cprtya4df3ZtgzKglNQP99IO0G1UGhWqR%2BTCWHI7rWC1GbAvlR6F2BJ%2FlMsCzp8g8W%2BDPCbuE%2FNNE6Aau%2B7IKf1FAb43toJZks6EGXsb2MJV%2FdvWxZ5bfibvPbFvK%2BqXBw14AxvbQISwm0mfr8DA8QFNBaHJOiMNb8gpoGOqgBJs3gG1t3ALnloRoIoHB8QL3MlPMpTIj1v%2FZmaDYhelvFYIOYzE1ET5fSHuHC4E50R9nf%2FOXToD3dA3GUOSfJ2yI0fr9t6Im7TkHtAZeTT1fF0N8yLDJW4ndBe1BnsUBfW%2B9OWMAuhzSkRwZ212K7QX3LHDOprjAj2xw%2BW5vQmA5d3YNCx2PSgLt%2F9mrMdsAmdrq5AUq1F69mTaGfNHJQBGMkrx3lSKPe&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221008T010429Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYY5RSKDNU%2F20221008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=a8cee51c18541c2eb98b6a85b5ea5fa0c9f12055b9402f3db78a590c3a20c6ac&hash=113b1943936da1c0b06ee7a6b9ecbb0afa78138830dd4dd57d92a215ed8c2632&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S187705092200093X&tid=spdf-cd28efe5-a8a7-4766-aa37-4d6fcb89dd4a&sid=a9e72241262dd54d3a192e03a5d012da13bagxrqb&type=client&ua=5154520400555356500b&rr=756af6794ee4a05d\n",
        "\n",
        "* https://docs.google.com/document/d/1FnK_zLTJKksuwhofKQlKJwoPoeO0xXMZ27NGqPdZLMA/edit\n",
        "\n",
        "\n",
        "12/16\n",
        "\n",
        "\n",
        "*   Look into the last two lines of the pseudocode\n",
        "*   Apply the pseudocode... goodluck lol\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Kdhv9pYbDzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ip016EejbRQt"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize Comment_Matrix \n",
        "# C = array of size n Ã— p\n",
        "\n",
        "# 2. Embed the comment matrix with Glove \n",
        "# Ce = apply glove to C\n",
        "\n",
        "# grab embedding for each word and put in an array\n",
        "# Ce = array of n x p x e, where e is the embedding size\n",
        "\n",
        "\n",
        "# 3. initialize Bi-LSTM and Bi-GRU... and run models. will take awhile\n",
        "\n",
        "# pooling = [globalMax1D, globalAvg1D]\n",
        "# Lc = []\n",
        "# filtersizes = [4,6] -- 4 and 6 are the filter sizes that the paper determined\n",
        "# NumOfFilters = 32 -- this was determined by the paper as well\n",
        "\n",
        "# models = [Bi-LSTM, Bi-GRU]\n",
        "# for model in models:\n",
        "#   apply model on Ce to obtain left and right (bidirectional) contexts, h -> and h <-\n",
        "#   h = [h ->, h <-] \n",
        "#   u = tanh(W*h + b); u is the importance of a word\n",
        "#   uT = u transposed\n",
        "#   uw = randomly initialized and learned in the training phase\n",
        "#   numerator = exp(uT * uw)\n",
        "#   denominator = 0\n",
        "#   for word in sentence:\n",
        "#     denominator = denominator + exp(uT word * uw)\n",
        "#   alpha = numerator / denominator\n",
        "#   S = 0\n",
        "#   for word in sentence:\n",
        "#     S = S + alpha word * h word\n",
        "#   \n",
        "#   CNN1 = CNN(S, 4, 32) -- filter size 4, number of filters 32\n",
        "#   CNN1MAX = Apply max to CNN1\n",
        "#   CNN1AVG = Apply avg to CNN1\n",
        "#   Lc.append(CNN1MAX, CNN1AVG)\n",
        "\n",
        "#   CNN2 = CNN(S, 6, 32) -- filter size 6, number of filters 32\n",
        "#   CNN2MAX = Apply max to CNN2\n",
        "#   CNN2AVG = Apply avg to CNN2\n",
        "#   Lc.append(CNN2MAX, CNN2AVG)\n",
        "# Lc is [CNN1Max, CNN1AVG, CNN2MAX, CNN2AVG]  \n",
        "# \n",
        "# hp= apply(Lc, batchnormalization) -- apply bath normalization to Lc\n",
        "# hd = relue(W*hp + b) -- W and b are parameters\n",
        "# sigmoid(hd) -- feed hd into a sigmoid for binary classification; NOTE: maybe look into which sigmoid function\n",
        "# Update parameters of the \"model\" using the binary cross-entropy loss function witht the Adam method; NOTE: figure out which model!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yN_8WoE5HPcU"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMfQDSdlG9Au"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive Init"
      ],
      "metadata": {
        "id": "TlQtyxq8Maqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugkvor0TMhA5",
        "outputId": "4cefbacf-1640-4ea1-ebea-f4c86f5287d6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install contractions"
      ],
      "metadata": {
        "id": "RccMdBgtXdbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import string\n",
        "import numpy as np\n",
        "import contractions\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "metadata": {
        "id": "ka94MG56P1eq"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "inputDF = getDF(\"/content/drive/MyDrive/Star Data v2/data/reviews_Kindle_Store_5.json(3).gz\")"
      ],
      "metadata": {
        "id": "2MaCwSjYMc14"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputDF[\"reviewText\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "YHSHLcgHNOOP",
        "outputId": "43a663bf-c8a5-484f-b47f-61e7d7f2831b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I enjoy vintage books and movies so I enjoyed reading this book.  The plot was unusual.  Don't think killing someone in self-defense but leaving the scene and the body without notifying the police or hitting someone in the jaw to knock them out would wash today.Still it was a good read for me.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Initialize Comment_Matrix "
      ],
      "metadata": {
        "id": "ySxFTZAxAAyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#n = 982619\n",
        "#p = 100\n",
        "\n",
        "#remove everything but the review\n",
        "#clean the review data\n",
        "#add paddiang and create matrix\n"
      ],
      "metadata": {
        "id": "NXvQy57DADOF"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanSentence(sentence):\n",
        "  for chr in string.punctuation:\n",
        "    if chr == \"'\" or chr == '-':\n",
        "      pass\n",
        "    else: \n",
        "      sentence = sentence.replace(chr, \" \")\n",
        "  sentence = sentence.replace(\"  \", \" \")\n",
        "  sentence = contractions.fix(sentence)\n",
        "  return sentence\n",
        "\n",
        "def turnIntoArray(sentence):\n",
        "  return sentence.split(\" \")\n",
        "\n",
        "def addPadding(wordArray, p):\n",
        "  count = len(wordArray) \n",
        "  if count == p:\n",
        "    return wordArray\n",
        "  elif count > p:\n",
        "    return wordArray[:p]\n",
        "  else:\n",
        "    return wordArray + ([\"\"] * (p - len(wordArray)))\n",
        "\n",
        "def createWordMatrix(sentence):\n",
        "  return addPadding(turnIntoArray(cleanSentence(sentence)), 100)\n",
        "\n"
      ],
      "metadata": {
        "id": "mFCpJQeJOSpb"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = list(inputDF[\"reviewText\"])\n",
        "commentMatrix = []\n",
        "for review in reviews:\n",
        "  commentMatrix.append(createWordMatrix(review))"
      ],
      "metadata": {
        "id": "q1VaDDxAQytM"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Todo after 01/06/2023\n",
        "# Embed the previous comment matrix with glove size 100 (step 2)\n",
        "# Stuff from the pseudocode (Step 3)"
      ],
      "metadata": {
        "id": "Va0VNm06W6wN"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file = datapath('/content/drive/MyDrive/Star Data v2/glove.6B/glove.6B.100d.txt')\n",
        "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ],
      "metadata": {
        "id": "Pch6jYvFVhsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150c7cd1-8c5e-41eb-c77e-3aacec99dc3e"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400001, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ],
      "metadata": {
        "id": "9B9kLg9cFo2k"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def actual_get_vector(word):\n",
        "  try:\n",
        "    return model.get_vector(word.lower())\n",
        "  except:\n",
        "    # print('doesn\\'t exist: ',word.lower())\n",
        "    return np.zeros(shape=(100,))"
      ],
      "metadata": {
        "id": "eDk7LWLFF_jI"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_comment_matrix = []\n",
        "for sentence in commentMatrix:\n",
        "  embedded_sentence = []\n",
        "  for word in sentence:\n",
        "    embedded_sentence.append(actual_get_vector(word))\n",
        "  embedded_comment_matrix.append(embedded_sentence)"
      ],
      "metadata": {
        "id": "baSTxmmEKWdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Todo after 01/20/2023\n",
        "# Embed the previous comment matrix with glove size 100 (step 2) -- run this locally or find a more efficient way to run the code above.. good luck lol\n",
        "# Stuff from the pseudocode (Step 3)"
      ],
      "metadata": {
        "id": "VZrfP1_QZjtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}